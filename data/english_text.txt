Hurricane Michael was the third-most intense Atlantic hurricane to make landfall in the United States in terms of pressure, behind the 1935 Labor Day hurricane and Hurricane Camille of 1969. It was also the strongest in terms of maximum sustained wind speed to strike the contiguous United States since Andrew in 1992. In addition, it was the strongest on record in the Florida Panhandle, and was the fourth-strongest landfalling hurricane in the contiguous United States, in terms of wind speed.

The thirteenth named storm, seventh hurricane, and second major hurricane of the 2018 Atlantic hurricane season, Michael originated from a broad low-pressure area that formed in the southwestern Caribbean Sea on October 2. The disturbance became a tropical depression on October 7, after nearly a week of slow development. By the next day, Michael had intensified into a hurricane near the western tip of Cuba as it moved northward. The hurricane strengthened rapidly in the Gulf of Mexico, reaching major hurricane status on October 9, peaking at a Category 4 hurricane on the Saffir–Simpson scale. Approaching the Florida Panhandle, Michael attained peak winds of 155 mph (250 km/h) as it made landfall near Mexico Beach, Florida, on October 10, becoming the first to do so in the region as a Category 4 hurricane, and making landfall as the strongest storm of the season. As it moved inland, the storm weakened and began to take a northeastward trajectory toward Chesapeake Bay, weakening to a tropical storm over Georgia, and transitioning into an extratropical cyclone off the coast of the Mid-Atlantic states on October 12. Michael subsequently strengthened into a powerful extratropical cyclone and eventually impacted the Iberian Peninsula, before dissipating on October 16.

By October 16, at least 48 deaths had been attributed to the storm, including 33 in the United States and 15 in Central America. Insurance losses due to Michael in the United States are estimated to be at least $6 billion (2018 USD).[1] As a tropical depression, the storm caused extensive flooding in Central America in concert with a second disturbance over the eastern Pacific Ocean. In Cuba, the hurricane's winds left over 200,000 people without power as the storm passed to the island's west. Along the Florida panhandle, the cities of Mexico Beach and Panama City suffered the worst of Michael, with catastrophic damage reported due to the extreme winds and storm surge. Numerous homes were flattened and trees felled over a wide swath of the panhandle. A maximum wind gust of 129 mph (208 km/h) was measured at Tyndall Air Force Base near the point of landfall. As Michael tracked across the Southeastern United States, strong winds caused extensive power outages across the region

Early on October 2, the National Hurricane Center (NHC) began monitoring a broad area of low pressure that had developed over the southwestern Caribbean Sea.[2] While strong upper-level winds initially inhibited development, the disturbance gradually became better organized as it drifted generally northward and then eastward toward the Yucatán Peninsula. By October 6, the disturbance had developed well-organized deep convection, although it still lacked a well-defined circulation. The storm was also posing an immediate land threat to the Yucatán Peninsula and Cuba. Thus, the NHC initiated advisories on Potential Tropical Cyclone Fourteen at 21:00 UTC that day.[3][4] By the morning of October 7, radar data from Belize found a closed center of circulation, while satellite estimates indicated a sufficiently organized convective pattern to classify the system as a tropical depression.[5] The newly-formed tropical cyclone then quickly strengthened into Tropical Storm Michael at 16:55 UTC that day.[6] The nascent system meandered before the center relocated closer to the center of deep convection, as reported by reconnaissance aircraft that was investigating the storm.[7] Despite moderate vertical wind shear, Michael proceeded to strengthen quickly, becoming a high-end tropical storm early on October 8, as the storm's cloud pattern became better organized.[8] Continued intensification occurred, and Michael attained hurricane status later on the same day.[9]
The eye of Hurricane Michael seen from the International Space Station on October 10

Shortly afterwards, rapid intensification began to ensue and very deep bursts of convection were noted within the eyewall of the growing hurricane, as it passed through the Yucatán Channel into the Gulf of Mexico late on October 8, clipping the western end of Cuba. Meanwhile, a 35 nmi (65 km) wide eye was noted to be forming.[10] The intensification process accelerated on October 9, with Michael becoming a major hurricane at 21:00 UTC that day.[11] In addition, the central pressure in the eye was noted to have dropped about 20 mb (0.59 inHg) in the span of 6 hours, into the first hours of October 10.[12] Rapid intensification continued throughout the day as a well-defined eye appeared, culminating with Michael achieving its peak intensity at 18:00 UTC that day as a high-end Category 4 hurricane, with maximum sustained winds of 155 mph (250 km/h) and a minimum central pressure of 919 mbar (27.14 inHg), just below Category 5 intensity, as it made landfall on the Gulf Coast of the United States near Mexico Beach, Florida, ranking by pressure as the third-most intense Atlantic hurricane to ever make landfall in the United States.[13]

Once inland, Michael began to rapidly weaken, as it moved over the inner Southeastern United States, with the eye dissipating from satellite view, weakening to a tropical storm roughly twelve hours after it made landfall.[14] Moving into the Carolinas early on October 11, the inner core collapsed as rainfall became prominent to the north of the center. Later that day, Michael began to show signs of becoming an extratropical cyclone, as it accelerated east-northeastward toward the Mid-Atlantic coastline, with cooler air beginning to wrap into the elongating circulation, due to an encroaching frontal zone.[15] Afterward, during the early hours of October 12, Michael began to restrengthen while moving off the coast, due to baroclinic forcing. Shortly afterward, Michael completed its extratropical transition around 09:00 UTC on the same day, as the storm became fully embedded within the frontal zone.[16] Michael subsequently accelerated towards the east, strengthening into a powerful extratropical cyclone by October 14.[17] On October 15, Michael's extratropical remnant approached the Iberian Peninsula and turned sharply towards the southeast,[18] making landfall on Portugal early on October 16. Following landfall, Michael's remnant quickly weakened, dissipating later on the same day.[19] 

Michael made landfall as a high-end Category 4 hurricane, with maximum sustained winds of 155 mph (250 km/h), at 12:15 CDT (17:15 UTC) on October 10, in Mexico Beach, Florida, and near Tyndall Air Force Base.[34] In Mexico Beach, many homes were flattened or completely swept away by storm surge and several roofs landed on U.S. Route 98. The National Guard rescued about 20 people, while it was estimated that as many as 285 residents of the small town may have stayed. Major damage occurred at Tyndall Air Force Base, with nearly every house at the base suffering roof damage. A wind gust of 129 mph (208 km/h) was recorded at the base. Debris on Interstate 10 resulted in the roadway being closed between Lake Seminole and Tallahassee, a distance of about 80 mi (130 km). In Tallahassee, a number of trees fell across the city and approximately 110,000 businesses and homes were left without electricity, while a sewer system failed. In Chattahoochee, the Florida State Hospital – the oldest and largest psychiatric hospital in the state – became isolated, forcing aid to be dropped by helicopter. Four deaths occurred in Gadsden County,[35] and another three deaths occurred in Marianna, Jackson County. A body was discovered by rescue crews in Mexico Beach on October 12.[36] By October 17, a total of 23 people were officially confirmed to have been killed by the hurricane in Florida.[27]


Panama City's proximity to the eyewall and its associated strongest winds and storm surge resulted in the majority, if not all, of the city's television and radio stations being rendered inoperable during and after the height of landfall. NBC affiliate WJHG-TV (channel 7) and CBS affiliate WECP-LD (channel 18) – both owned by Gray Television – were the first broadcast outlets to be affected, as their studio/transmitter link tower (which also housed the main transmitter for ESPN Radio affiliate WGSX [104.3 FM]) collapsed around 12:00 p.m. CDT on October 10, and parts of the roof of WJHG/WECP's studio facility were torn off.[46]

ABC affiliate WMBB (channel 13) subsequently lost its main power and its backup generator around 12:15 p.m. CDT. (WMBB provided live coverage from its Nexstar Media Group-owned sister stations WFLA-TV in Tampa and WDHN in Dothan, Alabama (the latter of which, alongside Gray-owned CBS affiliate WTVY was also knocked off the air later that same day as Michael passed over the Wiregrass Region) on its website and cable feed afterward.) iHeartMedia's Panama City radio cluster – WDIZ (590 AM), WEBZ (99.3 FM), WFLF-FM (94.5), WFSY (98.5 FM) and WPAP (92.5 FM) – switched to programming from the company's Tallahassee cluster as Michael made landfall, before the STL tower at their facility was felled; station staff were reported trapped at the facility due to flooding that also crept into the building.[46]

The respective radio clusters owned by Powell Broadcasting (WASJ [105.1 FM], WKNK [103.5 FM], WPFM [107.9 FM] and WRBA [95.9 FM]) and Magic Broadcasting II (WILN [105.9 FM], WWLY [100.1 FM], WYOO [101.1 FM] and WYYX [97.7 FM]), and Gulf Coast State College-owned WKGC-AM-FM (1480 and 90.7) were also knocked off the air due to tower collapses or lost power. (WPAP and WFSY returned to the air during the evening of October 10 with locally originated coverage, with WKGC following suit with programming originating from the Bay County Emergency Operations Center and using WMBB staff.)[46] Due to “catastrophic damage” sustained to its broadcast facility on Panama City Beach after it received water damage when a collapsed portion of its 150 feet (46 m) STL tower punched a hole in the roof of the building, Powell Broadcasting announced on October 13 that it would cease the operations of all of its Panama City radio stations.[47] 

Russian President Vladimir Putin described the attack as a "tragic event" and offered his condolences to the victims' families.[33] Putin placed the blame of the attack on social media and the internet, and the "tragic events in schools in the US".[2]

Crimean Prime Minister Sergey Aksyonov announced that there would be three days of mourning in Crimea. Crimean State Council speaker Vladimir Konstantinov announced that the victims' families would receive financial compensation, with preliminary discussions suggesting that the payments would be 1 million rubles (USD 17140.9) from the federal and local budget.[16] The Ukrainian Prosecutor General's Office of the Autonomous Republic of Crimea initiated criminal proceedings under the article "act of terrorism" investigating the attack.[34] Ukrainian President Petro Poroshenko expressed condolences to the victims, whom he described as Ukrainian citizens.[34][nb 1] 

The Japanese occupation government immediately began issuing military banknotes for use in the occupied Indies, as had previously been done in other occupied territories.[3] These first banknotes were printed in Japan, and issued by the Ministry of Finance.[5][3] This issue formally retained the gulden name, though in common indigene parlance it was called oeang Djepang (Japanese money) or oeang pisang (banana money, for the prominent bananas on the ten gulden note).[5] Each gulden (or, later, roepiah) consisted of 100 cents (sen).[6]

After the occupation began, the Japanese military government ruled that, as of 11 March 1942, the only valid currency in the region were military banknotes and existing colonial gulden.[4] Soon, however, they had begun replacing the pre-war currency at par.[5] They soon required that all extant Dutch currency be exchanged for the occupation issue. This policy, however, was not implemented very strictly, and pre-war currency was widely hoarded, even in the internment camps.[5]

In March 1943, the Japanese occupation government ceased issuing military notes; at the time, military currency to the value of 353 million gulden was in circulation.[7] Printing operations were moved to Kolff in Batavia (now Jakarta), Java.[5] These banknotes, which experienced no change in appearance, were issued by the Southern Development Bank (SDB), which had been established the preceding year and was managed by Yokohama Specie Bank and Bank of Taiwan.[8]

Under the SDB, an increasingly large amount of currency was issued; the economist Shibata Yoshimasa writes that, by the end of 1943, the total circulation had almost doubled to 674 million gulden, reaching almost two billion by the end of 1944.[9] This increase in circulation was followed by a drastic increase in inflation. Ultimately, this currency, renamed the roepiah for the 1944 issue,[10] was widely used but highly deprecated.[5] 


The Japanese forces surrendered on 15 August, and two days later the Republic of Indonesia proclaimed its independence.[6] Initially, the widely available Japanese-issued roepiah were accepted as legal tender, together with the pre-war gulden, in both areas controlled by the Netherlands and those under Republican rule;[5] indeed, the Netherlands Indies Civil Administration (NICA) printed more to deal with the costs of reestablishing Dutch administration in the area, though this also led to a continued increase in inflation.[11] Japanese issued notes were not, however, at par with pre-war gulden; in Java, the exchange rate was 10:1 to 12:1.[12]

On 6 March 1946, Dutch-controlled areas replaced the Japanese-issue roepiah with the NICA-issued gulden, giving an official exchange rate of 3 NICA gulden to 100 Japanese roepiah.[13] The Republican government followed suit on 30 October 1946, replacing the occupation currency with Oeang Repoeblik Indonesia (ORI) at an official rate of 50 Japanese roepiah for 1 ORI.[14][15] However, owing to the ongoing Indonesian National Revolution and the resulting chaotic monetary landscape, Japanese-issued bills remained in use into 1949.[6]

The Indonesian Minister of Finance, Alexander Andries Maramis, estimated in 1946 that the Japanese had put some 2.2 billion roepiah into circulation by the end of the occupation.[5] Yoshimasa gives a considerably higher amount, over 3.1 billion.[9] The Australian historian Robert Cribb, meanwhile, writes that the Japanese issued considerably more than they recorded, and that – combined with money printed after the Japanese surrender – the actual total could be between 3.5 and 8 billion, with only 2.7 billion issued during the occupation.[11] 

An electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. As the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, electron microscopes have a higher resolving power than light microscopes and can reveal the structure of smaller objects. A scanning transmission electron microscope has achieved better than 50 pm resolution in annular dark-field imaging mode[1] and magnifications of up to about 10,000,000x whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000x.

Electron microscopes have electron optical lens systems that are analogous to the glass lenses of an optical light microscope.

Electron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, electron microscopes are often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the images. 


The SEM produces images by probing the specimen with a focused electron beam that is scanned across a rectangular area of the specimen (raster scanning). When the electron beam interacts with the specimen, it loses energy by a variety of mechanisms. The lost energy is converted into alternative forms such as heat, emission of low-energy secondary electrons and high-energy backscattered electrons, light emission (cathodoluminescence) or X-ray emission, all of which provide signals carrying information about the properties of the specimen surface, such as its topography and composition. The image displayed by an SEM maps the varying intensity of any of these signals into the image in a position corresponding to the position of the beam on the specimen when the signal was generated. In the SEM image of an ant shown below and to the right, the image was constructed from signals produced by a secondary electron detector, the normal or conventional imaging mode in most SEMs.

Generally, the image resolution of an SEM is lower than that of a TEM. However, because the SEM images the surface of a sample rather than its interior, the electrons do not have to travel through the sample. This reduces the need for extensive sample preparation to thin the specimen to electron transparency. The SEM is able to image bulk samples that can fit on its stage and still be maneuvered, including a height less than the working distance being used, often 4 millimeters for high-resolution images. The SEM also has a great depth of field, and so can produce images that are good representations of the three-dimensional surface shape of the sample. Another advantage of SEMs comes with environmental scanning electron microscopes (ESEM) that can produce images of good quality and resolution with hydrated samples or in low, rather than high, vacuum or under chamber gases. This facilitates imaging unfixed biological samples that are unstable in the high vacuum of conventional electron microscopes.
An image of an ant in a scanning electron microscope


In their most common configurations, electron microscopes produce images with a single brightness value per pixel, with the results usually rendered in grayscale.[13] However, often these images are then colorized through the use of feature-detection software, or simply by hand-editing using a graphics editor. This may be done to clarify structure or for aesthetic effect and generally does not add new information about the specimen.[14]

In some configurations information about several specimen properties is gathered per pixel, usually by the use of multiple detectors.[15] In SEM, the attributes of topography and material contrast can be obtained by a pair of backscattered electron detectors and such attributes can be superimposed in a single color image by assigning a different primary color to each attribute.[16] Similarly, a combination of backscattered and secondary electron signals can be assigned to different colors and superimposed on a single color micrograph displaying simultaneously the properties of the specimen.[17]

Some types of detectors used in SEM have analytical capabilities, and can provide several items of data at each pixel. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors, it is common to color code the signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal, which is not modified in any way.
Reflection electron microscope (REM)

In the reflection electron microscope (REM) as in the TEM, an electron beam is incident on a surface but instead of using the transmission (TEM) or secondary electrons (SEM), the reflected beam of elastically scattered electrons is detected. This technique is typically coupled with reflection high energy electron diffraction (RHEED) and reflection high-energy loss spectroscopy (RHELS).[citation needed] Another variation is spin-polarized low-energy electron microscopy (SPLEEM), which is used for looking at the microstructure of magnetic domains.[18]
Scanning transmission electron microscope (STEM)
Main article: Scanning transmission electron microscopy

The STEM rasters a focused incident probe across a specimen that (as with the TEM) has been thinned to facilitate detection of electrons scattered through the specimen. The high resolution of the TEM is thus possible in STEM. The focusing action (and aberrations) occur before the electrons hit the specimen in the STEM, but afterward in the TEM. The STEMs use of SEM-like beam rastering simplifies annular dark-field imaging, and other analytical techniques, but also means that image data is acquired in serial rather than in parallel fashion. Often TEM can be equipped with the scanning option and then it can function both as TEM and STEM.
Sample preparation
An insect coated in gold for viewing with a scanning electron microscope

Materials to be viewed under an electron microscope may require processing to produce a suitable sample. The technique required varies depending on the specimen and the analysis required:

    Chemical fixation – for biological specimens aims to stabilize the specimen's mobile macromolecular structure by chemical crosslinking of proteins with aldehydes such as formaldehyde and glutaraldehyde, and lipids with osmium tetroxide.
    Negative stain – suspensions containing nanoparticles or fine biological material (such as viruses and bacteria) are briefly mixed with a dilute solution of an electron-opaque solution such as ammonium molybdate, uranyl acetate (or formate), or phosphotungstic acid. This mixture is applied to a suitably coated EM grid, blotted, then allowed to dry. Viewing of this preparation in the TEM should be carried out without delay for best results. The method is important in microbiology for fast but crude morphological identification, but can also be used as the basis for high-resolution 3D reconstruction using EM tomography methodology when carbon films are used for support. Negative staining is also used for observation of nanoparticles.
    Cryofixation – freezing a specimen so rapidly, in liquid ethane, and maintained at liquid nitrogen or even liquid helium temperatures, so that the water forms vitreous (non-crystalline) ice. This preserves the specimen in a snapshot of its solution state. An entire field called cryo-electron microscopy has branched from this technique. With the development of cryo-electron microscopy of vitreous sections (CEMOVIS), it is now possible to observe samples from virtually any biological specimen close to its native state.[citation needed]
    Dehydration – or replacement of water with organic solvents such as ethanol or acetone, followed by critical point drying or infiltration with embedding resins. Also freeze drying.
    Embedding, biological specimens – after dehydration, tissue for observation in the transmission electron microscope is embedded so it can be sectioned ready for viewing. To do this the tissue is passed through a 'transition solvent' such as propylene oxide (epoxypropane) or acetone and then infiltrated with an epoxy resin such as Araldite, Epon, or Durcupan;[19] tissues may also be embedded directly in water-miscible acrylic resin. After the resin has been polymerized (hardened) the sample is thin sectioned (ultrathin sections) and stained – it is then ready for viewing.
    Embedding, materials – after embedding in resin, the specimen is usually ground and polished to a mirror-like finish using ultra-fine abrasives. The polishing process must be performed carefully to minimize scratches and other polishing artifacts that reduce image quality.
    Metal shadowing – Metal (e.g. platinum) is evaporated from an overhead electrode and applied to the surface of a biological sample at an angle.[20] The surface topography results in variations in the thickness of the metal that are seen as variations in brightness and contrast in the electron microscope image.
    Replication – A surface shadowed with metal (e.g. platinum, or a mixture of carbon and platinum) at an angle is coated with pure carbon evaporated from carbon electrodes at right angles to the surface. This is followed by removal of the specimen material (e.g. in an acid bath, using enzymes or by mechanical separation[21]) to produce a surface replica that records the surface ultrastructure and can be examined using transmission electron microscopy.
    Sectioning – produces thin slices of the specimen, semitransparent to electrons. These can be cut on an ultramicrotome with a glass or diamond knife to produce ultra-thin sections about 60–90 nm thick. Disposable glass knives are also used because they can be made in the lab and are much cheaper.
    Staining – uses heavy metals such as lead, uranium or tungsten to scatter imaging electrons and thus give contrast between different structures, since many (especially biological) materials are nearly "transparent" to electrons (weak phase objects). In biology, specimens can be stained "en bloc" before embedding and also later after sectioning. Typically thin sections are stained for several minutes with an aqueous or alcoholic solution of uranyl acetate followed by aqueous lead citrate.[22]
    Freeze-fracture or freeze-etch – a preparation method[23][24] particularly useful for examining lipid membranes and their incorporated proteins in "face on" view.[25] The fresh tissue or cell suspension is frozen rapidly (cryofixation), then fractured by breaking[26] or by using a microtome while maintained at liquid nitrogen temperature. The cold fractured surface (sometimes "etched" by increasing the temperature to about −100 °C for several minutes to let some ice sublime) is then shadowed with evaporated platinum or gold at an average angle of 45° in a high vacuum evaporator. The second coat of carbon, evaporated perpendicular to the average surface plane is often performed to improve the stability of the replica coating. The specimen is returned to room temperature and pressure, then the extremely fragile "pre-shadowed" metal replica of the fracture surface is released from the underlying biological material by careful chemical digestion with acids, hypochlorite solution or SDS detergent. The still-floating replica is thoroughly washed free from residual chemicals, carefully fished up on fine grids, dried then viewed in the TEM.
    Freeze-fracture replica immunogold labeling (FRIL) – the freeze-fracture method has been modified to allow the identification of the components of the fracture face by immunogold labeling. Instead of removing all the underlying tissue of the thawed replica as the final step before viewing in the microscope the tissue thickness is minimized during or after the fracture process. The thin layer of tissue remains bound to the metal replica so it can be immunogold labeled with antibodies to the structures of choice. The thin layer of the original specimen on the replica with gold attached allows the identification of structures in the fracture plane.[27] There are also related methods which label the surface of etched cells[28] and other replica labeling variations.[29]
    Ion beam milling – thins samples until they are transparent to electrons by firing ions (typically argon) at the surface from an angle and sputtering material from the surface. A subclass of this is focused ion beam milling, where gallium ions are used to produce an electron transparent membrane in a specific region of the sample, for example through a device within a microprocessor. Ion beam milling may also be used for cross-section polishing prior to SEM analysis of materials that are difficult to prepare using mechanical polishing.
    Conductive coating – an ultrathin coating of electrically conducting material, deposited either by high vacuum evaporation or by low vacuum sputter coating of the sample. This is done to prevent the accumulation of static electric fields at the specimen due to the electron irradiation required during imaging. The coating materials include gold, gold/palladium, platinum, tungsten, graphite, etc.
    Earthing – to avoid electrical charge accumulation on a conductively coated sample, it is usually electrically connected to the metal sample holder. Often an electrically conductive adhesive is used for this purpose.


A biological neural network is composed of a group or groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses[3] and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from neurotransmitter diffusion.

Artificial intelligence, cognitive modelling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.

Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.

Neural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence.
History

The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain[4] (1873) and William James[5] (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.
Computer simulation of the branching architecture of the dendrites of pyramidal neurons.[6]

For Bain,[4] every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain’s[4] theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.

James’s[5] theory was similar to Bain’s,[4] however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.

C. S. Sherrington[7] (1898) conducted experiments to test James’s theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation.

McCulloch and Pitts[8] (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.

In the late 1940s psychologist Donald Hebb[9] created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's B-type machines.

Farley and Clark[10] (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda[11] (1956).

Rosenblatt[12] (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos[13] (1975).

Neural network research stagnated after the publication of machine learning research by Minsky and Papert[14] (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975).[13]

The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland[15] (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.

Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.[16]
Neural networks and artificial intelligence
Main article: Artificial neural network

A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.

In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.

An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.[17]

One classical type of artificial neural network is the recurrent Hopfield network.

The concept of a neural network appears to have first been proposed by Alan Turing in his 1948 paper Intelligent Machinery in which called them "B-type unorganised machines".[18]

The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.

The tasks to which artificial neural networks are applied tend to fall within the following broad categories:

    Function approximation, or regression analysis, including time series prediction and modeling.
    Classification, including pattern and sequence recognition, novelty detection and sequential decision making.
    Data processing, including filtering, clustering, blind signal separation and compression.

Application areas of ANNs include nonlinear system identification [19] and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, "KDD"), visualization and e-mail spam filtering.
Neural networks and neuroscience

Theoretical and computational neuroscience is the field concerned with the theoretical analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.

The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
Types of models

Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.
Criticism

A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.

A. K. Dewdney, a former Scientific American columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool." (Dewdney, p. 82)

Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).

Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft,[20] to detecting credit card fraud[citation needed].

Technology writer Roger Bridgman commented on Dewdney's statements about neural nets:

    Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".

    In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[21]

In response to this kind of criticism, one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.[22]

Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).
Recent improvements
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2010) (Learn how and when to remove this template message)

While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine, acetylcholine, and serotonin on behaviour and learning.

Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.

Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution.[23] If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing,[24] because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.

Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning.[25] For example, multi-dimensional long short term memory (LSTM)[26][27] won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.

Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures,[28] similar to the 1980 Neocognitron by Kunihiko Fukushima,[29] and the "standard architecture of vision",[30] inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex.

Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications. [19]

Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition[31] and the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.[32] Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance[33] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU. 
